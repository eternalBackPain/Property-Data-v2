{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://www.onthehouse.com.au/odin/api/marketstats/markets/trends\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "# 1. State files and property types\n",
    "state_files = [\n",
    "    'NSW_suburbs.json',\n",
    "    'NT_suburbs.json',\n",
    "    'QLD_suburbs.json',\n",
    "    'SA_suburbs.json',\n",
    "    'TAS_suburbs.json',\n",
    "    'VIC_suburbs.json',\n",
    "    'WA_suburbs.json',\n",
    "    'ACT_suburbs.json',\n",
    "]\n",
    "PROPERTY_TYPES = [\"House\", \"Unit\"]\n",
    "TIME_PERIOD_YEARS = 5  # adjust if needed\n",
    "\n",
    "# 2. Load all suburb_postcodes into a dict keyed by state code\n",
    "state_entries = {}\n",
    "for filename in state_files:\n",
    "    state = os.path.splitext(filename)[0].split('_')[0]  # e.g. 'NSW'\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    entries = [\n",
    "        {\n",
    "            'suburb': entry['text'].split(',', 1)[0].strip().upper().replace(' ', '+'),\n",
    "            'postcode': entry['text'].split(',', 1)[1].strip()\n",
    "        }\n",
    "        for entry in data\n",
    "    ]\n",
    "    state_entries[state] = entries\n",
    "\n",
    "# 3. Sanity check\n",
    "for state, entries in state_entries.items():\n",
    "    print(f\"{state}: {len(entries)} entries\")\n",
    "    print(entries[:5], \"\\n\")\n",
    "\n",
    "# 4. Fetch trends for every state & property type\n",
    "for state, entries in state_entries.items():\n",
    "    for ptype in PROPERTY_TYPES:\n",
    "        records = []\n",
    "        for entry in entries:\n",
    "            suburb = entry['suburb']\n",
    "            postcode = entry['postcode']\n",
    "            url = (\n",
    "                f\"{BASE_URL}/{suburb}/{state}/{postcode}\"\n",
    "                f\"?propertyType={ptype}&timePeriod={TIME_PERIOD_YEARS}\"\n",
    "            )\n",
    "            try:\n",
    "                resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"[ERROR]   {state} {ptype} ‚Äì {suburb}, {postcode} ‚Üí {e}\")\n",
    "                continue\n",
    "            except ValueError:\n",
    "                print(f\"[ERROR]   {state} {ptype} ‚Äì {suburb}, {postcode} ‚Üí invalid JSON\")\n",
    "                continue\n",
    "\n",
    "            records.append({\"suburb\": suburb, \"data\": data})\n",
    "            print(f\"[SUCCESS] {state} {ptype} ‚Äì {suburb}, {postcode}\")\n",
    "            time.sleep(1)  # polite pause\n",
    "\n",
    "        out_fname = f\"market_trends_{state.lower()}_{ptype.lower()}s.json\"\n",
    "        with open(out_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚Üí Saved {len(records)} records to {out_fname}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84095e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. grab every trends JSON your fetch-script produces\n",
    "#    (e.g. market_trends_nsw_houses.json, market_trends_vic_units.json, etc.)\n",
    "json_files = glob.glob(\"market_trends_*_*.json\")\n",
    "\n",
    "for json_file in json_files:\n",
    "    print(f\"\\nüõ†  Processing {json_file}...\")\n",
    "    payload = json.load(open(json_file, encoding=\"utf-8\"))\n",
    "    records = []\n",
    "\n",
    "    # 2. flatten into a long form\n",
    "    for loc in payload:\n",
    "        suburb = loc.get(\"suburb\", \"<UNKNOWN>\")\n",
    "        series_list = loc.get(\"data\", {}).get(\"seriesResponseList\")\n",
    "        if series_list is None:\n",
    "            print(f\"  ‚ö†Ô∏è  Missing seriesResponseList for {suburb!r}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        for metric in series_list:\n",
    "            locality = metric.get(\"localityName\", \"<NO_NAME>\")\n",
    "            postcode = metric.get(\"postcodeName\", \"<NO_CODE>\")\n",
    "            loc_id = f\"{locality}, {postcode}\"\n",
    "            # sanitize metricType for column names\n",
    "            m = metric[\"metricType\"].replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "            for point in metric.get(\"seriesDataList\", []):\n",
    "                d = pd.to_datetime(point[\"dateTime\"]).strftime(\"%Y%m%d\")\n",
    "                records.append({\n",
    "                    \"Locality\": loc_id,\n",
    "                    \"Metric\":   m,\n",
    "                    \"Date\":     d,\n",
    "                    \"Value\":    point[\"value\"]\n",
    "                })\n",
    "\n",
    "    flat_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    # 3. optional: inspect any exact duplicates\n",
    "    dups = flat_df[flat_df.duplicated([\"Locality\", \"Metric\", \"Date\"], keep=False)]\n",
    "    if not dups.empty:\n",
    "        print(\"  ‚ö†Ô∏è  Found duplicate rows; here they are:\")\n",
    "        print(dups)\n",
    "\n",
    "    # 4. drop duplicates and pivot\n",
    "    flat_df = flat_df.drop_duplicates([\"Locality\", \"Metric\", \"Date\"])\n",
    "    wide = flat_df.pivot_table(\n",
    "        index=\"Locality\",\n",
    "        columns=[\"Metric\", \"Date\"],\n",
    "        values=\"Value\",\n",
    "        aggfunc=\"first\"\n",
    "    )\n",
    "    wide.columns = [f\"{metric}_{date}\" for metric, date in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "\n",
    "    # 5. save out as CSV next to the JSON\n",
    "    csv_name = os.path.splitext(json_file)[0] + \".csv\"\n",
    "    wide.to_csv(csv_name, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"  ‚úÖ  Saved {csv_name} (shape: {wide.shape})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
