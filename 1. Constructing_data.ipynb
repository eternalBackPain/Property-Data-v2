{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823b64f4",
   "metadata": {},
   "source": [
    "# Constructing a property dataset for Australian investors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5633dcc",
   "metadata": {},
   "source": [
    "onthehouse.com.au provides suburb level property data which provides insights for all suburbs in Australia. I'm going to try and construct a dataset from this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511612a5",
   "metadata": {},
   "source": [
    "## Exposing the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911cf35",
   "metadata": {},
   "source": [
    "The API is quite easy to understanding.\n",
    "\n",
    "Here is an example of the request for the insights of Bonnyrigg (a NSW state): `https://www.onthehouse.com.au/odin/api/marketstats/markets/trends/BONNYRIGG/NSW/2177?propertyType=House&timePeriod=5`\n",
    "\n",
    "All we need is to insert `Bonnyrigg` (with `+` if there's a space in the suburb name) and its postcode `2177`.\n",
    "\n",
    "Both elements can be scrapped in full from these links (for each suburb): `https://www.onthehouse.com.au/suburb-research/nsw` \n",
    "\n",
    "I will first scrape all suburb names and postcodes, and then plug them into the API and construct a database from that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65392158",
   "metadata": {},
   "source": [
    "## Gathering suburb names and postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05707ad3",
   "metadata": {},
   "source": [
    "From `https://www.onthehouse.com.au/suburb-research/nsw`, there is a long list of these suburbs and their postcodes.\n",
    "\n",
    "I used this script on each page (there were 10 pages for NSW) to extract the JSON data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84decd",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// const listItems = Array.from(document.querySelectorAll('ul.d-sm-flex.flex-wrap.StateSuburbList__ssSuburblist--NizcF > li'));\n",
    "// const extractedData = listItems.map(li => {\n",
    "//   const anchorTag = li.querySelector('a');\n",
    "//   if (anchorTag) {\n",
    "//     return {\n",
    "//       href: anchorTag.href,\n",
    "//       text: anchorTag.textContent.trim()\n",
    "//     };\n",
    "//   }\n",
    "//   return null;\n",
    "// }).filter(item => item !== null);\n",
    "\n",
    "// console.log(extractedData);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73916b",
   "metadata": {},
   "source": [
    "The JSON file `NSW_suburbs.json` is the result of this exercise.\n",
    "\n",
    "Now to create an array in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ea37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('NSW_suburbs.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "suburb_postcodes = [\n",
    "    {\n",
    "        'suburb': name.strip().upper().replace(' ', '+'),\n",
    "        'postcode': code.strip()\n",
    "    }\n",
    "    for entry in data\n",
    "    for name, code in (entry['text'].split(',', 1),)\n",
    "]\n",
    "\n",
    "print(suburb_postcodes[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6ffc2",
   "metadata": {},
   "source": [
    "## Scraping the analytics data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5a69d",
   "metadata": {},
   "source": [
    "Before scraping, I want to make sure that my IP address wotn get banned.\n",
    "\n",
    "First, I'll check if the site has a robots.txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url(\"https://www.onthehouse.com.au/robots.txt\")\n",
    "rp.read()\n",
    "\n",
    "url = \"https://www.onthehouse.com.au/odin/api/marketstats/markets/trends/AARONS+PASS/NSW/2850?propertyType=House&timePeriod=5\"\n",
    "print(rp.can_fetch(\"*\", url))  # True means robots.txt permits it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8150890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "resp = requests.get(url, headers=headers)\n",
    "print(resp.status_code, resp.headers.get(\"Retry-After\"))\n",
    "\n",
    "time.sleep(1)  # don’t hammer the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b397d38",
   "metadata": {},
   "source": [
    "All looks ok to scrape, so lets do it (TAKES ABOUT AN HOUR AND A HALF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c1884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://www.onthehouse.com.au/odin/api/marketstats/markets/trends\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "all_trends = []\n",
    "\n",
    "for entry in suburb_postcodes:\n",
    "    suburb   = entry['suburb']\n",
    "    postcode = entry['postcode']\n",
    "    url = f\"{BASE_URL}/{suburb}/NSW/{postcode}?propertyType=House&timePeriod=5\"\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        record = resp.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[ERROR]   {suburb}, {postcode} → {e}\")\n",
    "        continue\n",
    "    except ValueError:\n",
    "        print(f\"[ERROR]   {suburb}, {postcode} → invalid JSON\")\n",
    "        continue\n",
    "\n",
    "    # wrap the JSON under a 'data' key along with suburb name\n",
    "    all_trends.append({\n",
    "        'suburb': suburb,\n",
    "        'data':   record\n",
    "    })\n",
    "    print(f\"[SUCCESS] {suburb}, {postcode} → record added\")\n",
    "\n",
    "    time.sleep(1)  # polite pause\n",
    "\n",
    "print(f\"Fetched {len(all_trends)} records.\")\n",
    "\n",
    "with open(\"test_market_trends_NSW.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_trends, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e58c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "payload = json.load(open(\"test_market_trends_NSW.json\"))\n",
    "records = []\n",
    "\n",
    "for loc in payload:\n",
    "    # no longer using loc['suburb']\n",
    "    try:\n",
    "        series_list = loc['data']['seriesResponseList']\n",
    "    except KeyError:\n",
    "        suburb = loc.get('suburb', '<UNKNOWN>')\n",
    "        print(f\"  ⚠️  Missing seriesResponseList for suburb {suburb!r}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    for metric in series_list:\n",
    "        # build a combined Locality–Postcode identifier\n",
    "        locality = metric.get('localityName', '<NO_NAME>')\n",
    "        postcode = metric.get('postcodeName', '<NO_CODE>')\n",
    "        loc_id = f\"{locality}, {postcode}\"\n",
    "\n",
    "        # sanitize metric name for use as a column later\n",
    "        m = (metric['metricType']\n",
    "             .replace(' ', '_')\n",
    "             .replace('(', '')\n",
    "             .replace(')', '')\n",
    "        )\n",
    "\n",
    "        for point in metric.get('seriesDataList', []):\n",
    "            d = pd.to_datetime(point['dateTime']).strftime('%Y%m%d')\n",
    "            records.append({\n",
    "                'Locality': loc_id,\n",
    "                'Metric':   m,\n",
    "                'Date':     d,\n",
    "                'Value':    point['value']\n",
    "            })\n",
    "\n",
    "flat_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca32b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — optional: inspect any true duplicates —\n",
    "dups = flat_df[flat_df.duplicated(['Locality','Metric','Date'], keep=False)]\n",
    "if not dups.empty:\n",
    "    print(\"Found duplicate rows; here they are:\")\n",
    "    print(dups)\n",
    "\n",
    "# drop exact duplicates (if that’s safe for your use case)\n",
    "flat_df = flat_df.drop_duplicates(['Locality','Metric','Date'])\n",
    "\n",
    "# now pivot using pivot_table+aggfunc to guard against any remaining dupes\n",
    "wide = flat_df.pivot_table(\n",
    "    index='Locality',\n",
    "    columns=['Metric','Date'],\n",
    "    values='Value',\n",
    "    aggfunc='first'     # or 'mean' / sum, as makes sense\n",
    ")\n",
    "\n",
    "# flatten the MultiIndex columns\n",
    "wide.columns = [f\"{metric}_{date}\" for metric, date in wide.columns]\n",
    "wide = wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to CSV\n",
    "df.to_csv(\"market_trends_NSW.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
